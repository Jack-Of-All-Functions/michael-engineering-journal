# 8/26/2020

## Table of Contents

1. [Stand Up Notes](#stand-up-notes)
2. [Work Completed](#work-completed)
3. [Lessons Learned](#lessons-learned)
4. [Looking Ahead](#looking-ahead)
5. [PostgreSQL study](#postgres-study)
6. [Using PostgreSQL with Node.js](#using-postgresql-with-nodejs)

## Stand Up Notes

We discussed what we would be working on today. Our primary focus both individually and as a group is to read up on Postgres and maybe start designing our database schemas.

## Work Completed

- Updated API routes
- Updated readme to include descriptions and define usage of API routes
- Read a lot about PostgreSQL
- Implemented a basic script with PostgreSQL and Node.js to create 1,000,000 random entries in a table. The entire time to generate the entries and load them into the table was 386 seconds.

## Lessons Learned

One of my big blockers for the day was using async/await. I kept getting an error telling me that await could only be used within an async function. I understood that to mean that await must be used *in conjunction* with an async function, as opposed to *inside of* an async function. This was a big learning moment for me, and hopefully a mistake that I won't repeat again.

What **not** to do:

```
const someFunction = () => {
  const result = await somePromise();
  return result;
};
```

What **to** do:

```
const someFunction = await () => {
  const result = await somePromise();
  return result;
};
```

This was a major thorn in my side today, but it's an easy enough fix now that I understand the problem.

## Looking Ahead

Now that I'm at a point where I understand Postgres and how to use it within Node, I'm going to look for ways to optimize the time it takes to load files into the database.

Ideas to explore:

- Create JSON files with 100,000 entries each - The first benefit of this is that I won't have to wait forever while a single file with 10 million entries is being created. The second benefit is that I may be able to benefit from multithreading and drastically reduce the time it takes to seed my database.
- Explore multithreading - My intuition tells me that loading to the database will be done on an individual thread, which will negate the benefits of multithreading my processes. Even if this is the case, multithreading is a good skill to have, so I'm not too concerned about wasting time with this. In addition, I am currently ahead of where I need to be in the project, so I have time to burn.
- Figure out the schemas for my databases - **This takes priority**, as it is a deliverable for tomorrow. I'm not terribly concerned about this step, however, since the datasets seem to be fairly straightforward.

Upcoming Deadlines:

- 8/27
    - Primary Database Chosen - Complete. I will be using PostgreSQL
    - Recieve approval for API and Schema - Need to do
- 8/28
    - Write queries for your routes - Pending approval for schema first
    - Data generation script - Pending approval for schema first, some work done
- 8/31
    - Insert 1M records into DB - Pending approval for schema first, some work done
    - Query execution time with 1M records in DB - Pending approval for schema first. Need to find a tool to help with this.
- 9/2
    - Insert 10M records into DB - Pending approval for schema first, some work done.
    - Query execution time with 10M records in DB - Pending approval for schema first. Need to find a tool to help with this. 

## Postgres Study

[This has been moved to its own file](https://github.com/Jack-Of-All-Functions/michael-engineering-journal/blob/master/PostgreSQL-Study.md)

## Using PostgreSQL with Node.js

[This has been move to its own file](https://github.com/Jack-Of-All-Functions/michael-engineering-journal/blob/master/Using-Postgres-With-Node.md)
